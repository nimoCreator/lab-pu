# -*- coding: utf-8 -*-
"""Lab Pu 4

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rCKSpekbTNoOJ0DH-TMu3gpWHR51eiLE
"""

import os, pathlib
os.environ["GGUF_PATH"] = "/content/models/qwen1_5-7b-chat-q5_k_m.gguf"
gguf_path = pathlib.Path(os.environ["GGUF_PATH"])
assert gguf_path.exists(), f"Nie znaleziono pliku: {gguf_path}"
gguf_path

from llama_cpp import Llama

LLAMA_CTX = 4096
LLAMA_THREADS = 8
LLAMA_NGPU = 999

_llm = None

def get_llm() -> Llama:
    global _llm
    if _llm is None:
        _llm = Llama(
            model_path=os.environ["GGUF_PATH"],
            n_ctx=LLAMA_CTX,
            n_threads=LLAMA_THREADS,
            n_gpu_layers=LLAMA_NGPU,
            verbose=False,
        )
    return _llm

def pytaj_Qwen(prompt: str, temperature: float = 0.7, top_p: float = 0.95, max_tokens: int = 512) -> str:
    """
    Pyta lokalny model Qwen (GGUF przez llama-cpp-python).
    Zwraca czysty tekst odpowiedzi.
    """
    llm = get_llm()
    system = "Odpowiadaj krotko i rzeczowo po polsku."
    full_prompt = f"### System:\n{system}\n\n### Pytanie:\n{prompt}\n\n### Odpowiedz:\n"
    out = llm.create_completion(
        prompt=full_prompt,
        temperature=temperature,
        top_p=top_p,
        max_tokens=max_tokens,
    )
    return (out["choices"][0]["text"] or "").strip()

print("Test 1:", pytaj_Qwen("Ile jest kart w standardowej talii?"))
print("Test 2:", pytaj_Qwen("Podaj 3 najwieksze miasta w Polsce."))

import os
import json
import time
import pathlib
import requests
from datetime import datetime, timezone

API_KEY = os.environ.get("GEMINI_API_KEY", "AIzaSyDxEDC3E7G8oUIJPrcKOGNdrddM6AcSaCw")

MODEL_NAME = "gemini-2.5-flash"
API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={API_KEY}"

LOG_DIR = pathlib.Path("/content/logs_gemini")
LOG_DIR.mkdir(exist_ok=True)
LOG_FILE = LOG_DIR / f"chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"

SYSTEM_PROMPT = (
    "Jestes pomocnym asystentem. "
    "Odpowiadaj krotko i rzeczowo, po polsku."
)

GEN_CFG = {
    "temperature": 0.7,
    "topP": 0.9,
}

def now_iso() -> str:
    return datetime.now(timezone.utc).isoformat(timespec="seconds")

def append_log(role: str, content: str, meta: dict | None = None) -> None:
    entry = {"ts": now_iso(), "role": role, "content": content}
    if meta:
        entry.update(meta)
    with LOG_FILE.open("a", encoding="utf-8") as f:
        f.write(json.dumps(entry, ensure_ascii=False) + "\n")

def print_info(msg: str) -> None:
    print(f"\033[90m{msg}\033[0m")
def pytaj_Gemini(prompt: str, history: list[dict] | None = None, system_prompt: str | None = None) -> str:
    """
    WysyÅ‚a zapytanie do Gemini (REST) i zwraca tekst odpowiedzi.
    - history: lista wpisÃ³w w formacie [{"role": "system"|"user"|"assistant", "content": "..."}]
      (zgodnie z Twoim stylem â€” "system" traktujemy jak "user" po stronie API).
    - system_prompt: opcjonalny system prompt; domyÅ›lnie bierze SYSTEM_PROMPT.
    """
    if not API_KEY or API_KEY == "WSTAW_TUTAJ_SWÃ“J_KLUCZ":
        raise RuntimeError("Ustaw GEMINI_API_KEY w Å›rodowisku lub wpisz w zmiennÄ… API_KEY.")

    system_prompt = system_prompt or SYSTEM_PROMPT
    history = history or []

    contents = []
    if system_prompt:
        contents.append({"role": "user", "parts": [{"text": system_prompt}]})

    for msg in history:
        role = msg["role"]
        if role == "system":
            role = "user"
        elif role == "assistant":
            role = "model"
        contents.append({"role": role, "parts": [{"text": msg["content"]}]})

    contents.append({"role": "user", "parts": [{"text": prompt}]})

    payload = {
        "contents": contents,
        "generationConfig": GEN_CFG,
    }

    t0 = time.perf_counter()
    resp = requests.post(API_URL, json=payload, timeout=120)
    dt = round(time.perf_counter() - t0, 3)

    if not resp.ok:
        append_log("assistant", f"HTTP {resp.status_code}: {resp.text}", {"error": True})
        raise RuntimeError(f"HTTP {resp.status_code}: {resp.text}")

    data = resp.json()
    try:
        text = data["candidates"][0]["content"]["parts"][0]["text"].strip()
    except Exception:
        text = "[blad parsowania odpowiedzi API]"

    append_log("user", prompt)
    append_log("assistant", text, {"latency_s": dt})
    return text

print_info(f"Log do pliku: {LOG_FILE}")
ans = pytaj_Gemini("Powiedz ile jest kart w talii")
print("Gemini:", ans)

import json, time, pathlib, os, re
from datetime import datetime, timezone

# --- katalog i nazwa logu z data ---
LOGS_DIR = pathlib.Path("/content/logs")
LOGS_DIR.mkdir(parents=True, exist_ok=True)
LOG_TXT_PATH = LOGS_DIR / f"log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.txt"

def _now_iso():
    return datetime.now(timezone.utc).isoformat(timespec="seconds")

def _now_local_str():
    return datetime.now().strftime("%H:%M:%S")

def _append_log_txt(entry: dict):
    """Zapis na biezaco (flush), z lokalna data."""
    entry["data_local"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with LOG_TXT_PATH.open("a", encoding="utf-8") as f:
        f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        f.flush()

def _print_info(msg: str, t_start: float, t_last: float):
    """Szary komunikat: [HH:MM:SS | +X.XXs | total Y.YYs] wiadomosc"""
    now = time.perf_counter()
    delta = now - t_last
    total = now - t_start
    print(f"\033[90m[{_now_local_str()} | +{delta:.2f}s | total {total:.2f}s] {msg}\033[0m")
    return now  # zwroc znacznik czasu do nastepnego etapu

def pytaj_z_Krytykiem(pytanie: str, max_rounds: int = 5) -> str:
    """
    Petla:
      1) pytanie -> Qwen (lokalny)
      2) recenzja -> Gemini (STRICT JSON: {"czy_korekta": bool, "sugestia": str})
      3) jesli trzeba, Qwen dostaje (pytanie, poprzednia odpowiedz, sugestie) i poprawia
      4) koniec gdy czy_korekta == False lub osiagnieto max_rounds
    Log dopisywany na biezaco do /content/logs/log_YYYY-MM-DD_HH-MM-SS.txt
    Konsola pokazuje czas lokalny, czas etapu i czas calkowity.
    """
    review_sys = (
        "Zwracaj SCISLY JSON, BEZ komentarzy. Schemat:\n"
        '{ "czy_korekta": boolean, "sugestia": string }\n'
        "Oceniaj poprawnosc, scislosc i brak halucynacji. "
        "Jesli trzeba poprawic -> czy_korekta=true i w 'sugestia' podaj konkretne wskazowki. "
        "Jesli jest dobrze -> czy_korekta=false, a w 'sugestia' krotkie uzasadnienie."
    )

    print(f"ðŸ“„ logowanie do pliku: {LOG_TXT_PATH}")
    print(f"Ty: {pytanie}")
    _append_log_txt({"ts": _now_iso(), "etap": "start", "pytanie": pytanie})

    t_start = time.perf_counter()
    t_last = t_start

    current_input = pytanie
    final_answer = None

    for r in range(1, max_rounds + 1):
        # 1) Qwen
        t_last = _print_info(f"â†’ przekazano pytanie do Qwen (runda {r})", t_start, t_last)
        t_loc0 = time.perf_counter()
        odp = pytaj_Qwen(current_input)
        t_last = _print_info("âœ” Qwen odpowiedzial", t_start, t_last)
        _append_log_txt({
            "ts": _now_iso(),
            "etap": "lokalna_odpowiedz",
            "runda": r,
            "wejscie": current_input,
            "odpowiedz": odp,
            "latency_s": round(time.perf_counter() - t_loc0, 3),
        })

        # 2) Gemini (review)
        t_last = _print_info("â†’ przekazano do recenzji Gemini", t_start, t_last)
        t_gem0 = time.perf_counter()
        contents_for_review = (
            review_sys
            + "\n\nPYTANIE:\n" + pytanie
            + "\n\nODP_LOCAL:\n" + odp
        )
        txt = pytaj_Gemini(contents_for_review)
        t_last = _print_info("âœ” Gemini zwrocil recenzje", t_start, t_last)

        raw = txt.strip()
        if raw.startswith("```"):
            raw = re.sub(r"^```json\s*|\s*```$", "", raw, flags=re.DOTALL).strip()

        try:
            review = json.loads(raw)
        except Exception:
            review = {"czy_korekta": True, "sugestia": "Nie udalo sie sparsowac JSON."}

        need_fix = bool(review.get("czy_korekta", True))
        _append_log_txt({
            "ts": _now_iso(),
            "etap": "gemini_recenzja",
            "runda": r,
            "review": review,
            "latency_s": round(time.perf_counter() - t_gem0, 3),
        })

        if not need_fix:
            final_answer = odp
            break

        # 3) prompt poprawkowy dla Qwen
        sugestia = (review.get("sugestia") or "").strip()
        current_input = (
            f"{pytanie}\n\n"
            f"Twoja poprzednia odpowiedz:\n{odp}\n\n"
            f"Ulepsz odpowiedz zgodnie z sugestiami recenzenta:\n{sugestia}\n\n"
            f"Odpowiedz krotko i poprawnie."
        )

    if final_answer is None:
        final_answer = "(Nie uzyskano akceptu w limicie iteracji.)"

    print(f"âœ… Model: {final_answer}")
    _append_log_txt({"ts": _now_iso(), "etap": "final", "final_answer": final_answer})
    return final_answer

wynik = pytaj_z_Krytykiem("Czy taiwan to osobny kraj?", max_rounds=5)
print("Wynik koÅ„cowy:", wynik)

print("Log zapisany do:", str(LOG_TXT_PATH.resolve()))

